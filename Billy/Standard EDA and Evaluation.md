# EDA and Evaluation

This will be a summary of the EDA and (Billy's initial and final) evaluation of the German Traffic Sign dataset.

## EDA

The EDA conducted clearly shows large disparities in the data. The initial plot of class distributions demonstrate that class size varies immensely, with some classes having over 2000 images and others only containing roughly 200. This will lead to an underrepresentation of certain classes in the training set, and this issue may be clouded due to high accuracy being dominated by the larger classes. Following this, we generated random images from the training set to better understand the data. This was then repeated class-wise, making it easier to spot any common extremeties - lighting, blurriness, rotation, obstruction and so on. The differences in images drastically hinders our models capability of accurately determing the correct class type, perhaps explaining the difficulty in classifying certain classes. We then looked at the resolution of images in the training set and witnessed large variations in the height and width of images, further reinforcing our need for preprocessing, particularly resizing and normalisation, of the data. A brief analysis of pixel distribution was then conducted, and then we moved onto image augmentation. This concludes our EDA and, even without expert knowledge, it is clear what the issues are with said data and how we aim to overcome them. This naturally leads us into training the models, which will not be discussed here but can be found in either Neva or Gracie's folders, or in the report folder. A more developed EDA could examine specific details more in depth. The next section discusses the findings, impact, scope and journey of my evaluation.

## Evaluation

Initially, the basic evaluation was conducted on the validation set to better understand the models and how they performed. This became a little muddled after we looked at the values more in depth. EfficientNet would regularly never even attempt to classify certain classes, leading to recall, precision and f1-scores of 0 for those classes. Clearly this was an issue, and luckily enough this led to further fine tuning which resolved this issue. Standard confusion matrices were then calculated followed by generating misclassified images from both models. The formal evaluation starts of with running both models with their fine tuned hyperparameters on the test set. The results suggest that EfficientNet is the superior of the two models, with higher accuracy and top-5 accuracy, whilst ResNet18 reports a better loss. However, with reference to the accuracy figures of both models, achieving a score of roughly 70% is not ideal whatsoever given the dataset, and provides us with good reasoning to believe that both models have performed poorly (ask yourself: would you want to be in a self-driving car that correctly classifies the traffic sign only 70%? Probably not). The remaining evaluation was talked about in detail in the file, so we will not regurgitate it here. Scaling was not covered in this evaluation as that can be found in Leo's file and in the report. References were provided in the EDA and evaluation files respectively.
