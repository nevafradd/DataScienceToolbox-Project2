{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77f1b8c8",
   "metadata": {},
   "source": [
    "## Continuation of EDA and Evaluating Valuation Set on German Traffic Sign Dataset using EfficientNet and ResNet18\n",
    "### Billy Ryan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae243fe",
   "metadata": {},
   "source": [
    "This file is a continuation of the initial eda file, just with an update of validation set to see the increase in accuracy. Everything else is the same but I thought it was worth to see how the project evolved over time so I decided to keep both files separate as a good comparison tool. All of the EDA conducted remains the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9013a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import kagglehub\n",
    "from typing import List\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageEnhance\n",
    "import math\n",
    "from typing import List, Tuple\n",
    "from collections import Counter\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394b6945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download latest version of the dataset\n",
    "path = kagglehub.dataset_download(\"meowmeowmeowmeowmeow/gtsrb-german-traffic-sign\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f51bada",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.listdir(path))\n",
    "\n",
    "train_df_path =  os.path.join(path, \"Train.csv\")\n",
    "test_df_path =  os.path.join(path, \"Test.csv\")\n",
    "\n",
    "train_img_path =  os.path.join(path, \"Train\")\n",
    "test_img_path =  os.path.join(path, \"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7def2f",
   "metadata": {},
   "source": [
    "Much of the following EDA was lifted and/or inspired from Neva's and Gracie's work alongside many kaggle notebooks which have been referenced below, making alterations for ease of use and more thorough analysis. This file will not go into much detail as that has been done separately either in the report folder or in a markdown file in my (Billy) folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8929a53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(train_df_path)\n",
    "test_df = pd.read_csv(test_df_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af83d08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['ClassId'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7745f584",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = train_df['ClassId'].value_counts().sort_index()\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.bar(counts.index, counts.values)\n",
    "plt.title(\"Distribution of images per class\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Number of images\")\n",
    "plt.xticks(counts.index)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e95f0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = {\n",
    "    0:\"Speed Limit (20Km/hr)\", 1:\"Speed Limit (30Km/hr)\", \n",
    "    2:\"Speed Limit (50Km/hr)\", 3: \"Speed Limit (60Km/hr)\", \n",
    "    4: \"Speed Limit (70Km/hr)\", 5: \"Speed Limit (80Km/hr)\",\n",
    "    6: \"End of Speed Limit (80Km/hr)\", 7: \"Speed Limit (100Km/hr)\", \n",
    "    8: \"Speed Limit (120Km/hr)\", 9: \"No Passing\", \n",
    "    10: \"No Passing for trucks over 3.5 tons\", 11: \"Right of way\", \n",
    "    12: \"Priotity Road\", 13: \"Yeild right of way\",\n",
    "    14: \"Stop\", 15: \"Prohibited for all vehicles\",\n",
    "    16: \"Trucks and tractors over 3.5 tons prohibited\", 17: \"Entery prohibited\",\n",
    "    18: \"Danger\", 19: \"Single curve left\",\n",
    "    20: \"Single curve right\", 21: \"Double curve\",\n",
    "    22: \"Rough road\", 23: \"Slippery road\",\n",
    "    24: \"Road narrows\", 25: \"Construction side ahead\",\n",
    "    26: \"Signal lights ahead\", 27: \"Pedestrian crosswalk ahead\",\n",
    "    28: \"Children\", 29: \"Bicycle crossing\",\n",
    "    30: \"Unexpected ice danger\", 31: \"Wild animal crossing\",\n",
    "    32: \"End of restrection\", 33: \"Mandatory direction of travel right\",\n",
    "    34: \"Mandatory direction of travel left\", 35: \"Mandatory direction of travel ahead\",\n",
    "    36: \"Straight or right\", 37: \"Straight or left\",\n",
    "    38: \"Keep right\", 39: \"Keep left\",\n",
    "    40: \"Traffic circle\", 41: \"End of no passing zone cars\",\n",
    "    42: \"End of no passing zone vehicle over 3.5 tons\"\n",
    "}\n",
    "\n",
    "train_df[\"ClassName\"] = train_df['ClassId'].map(class_names)\n",
    "test_df[\"ClassName\"] = test_df['ClassId'].map(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb7d835",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Maximum images per class: {train_df['ClassName'].value_counts().max()} (Class: {train_df['ClassName'].value_counts().idxmax()})\")\n",
    "print(f\"Minimum images per class: {train_df['ClassName'].value_counts().min()} (Class: {train_df['ClassName'].value_counts().idxmin()})\")\n",
    "print(f\"Average images per class: {train_df['ClassName'].value_counts().mean():.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cccc27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 8))\n",
    "ax = sns.countplot(data=train_df, x=\"ClassName\", order=train_df[\"ClassName\"].value_counts().index, color=\"#5F98E2\")\n",
    "plt.xlabel(\"Class Name\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(rotation=90)\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container, fontsize=7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae49357",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_image_generator(class_id):\n",
    "    if class_id < 0 or class_id > 42:\n",
    "        raise ValueError(\"class_id must be between 0 and 42 inclusive.\")\n",
    "\n",
    "    folder = os.path.join(train_img_path, str(class_id))\n",
    "    image_files = [f for f in os.listdir(folder) if not f.startswith(\".\")]\n",
    "\n",
    "    filename = random.choice(image_files)\n",
    "    sample_path = os.path.join(folder, filename)\n",
    "\n",
    "    img = Image.open(sample_path)\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566afede",
   "metadata": {},
   "source": [
    "Now we can randomise images that are in our training dataset. This will help familiarise the user with the types of images we are dealing with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c315b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_class = random.randint(0, 42)\n",
    "img1 = random_image_generator(random_class)\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.imshow(img1)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594d2670",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_paths(filepath: str) -> Tuple[List[str], List[str]]:\n",
    "    image_paths = []\n",
    "    class_labels = []\n",
    "\n",
    "    for root, _, files in os.walk(filepath):\n",
    "        for filename in files:\n",
    "            if filename.lower().endswith((\".png\", \".jpg\", \".jpeg\", \".ppm\", \".bmp\")):\n",
    "                full_path = os.path.join(root, filename)\n",
    "                image_paths.append(full_path)\n",
    "\n",
    "                # grab class folder name\n",
    "                class_id = os.path.basename(root)\n",
    "                class_labels.append(class_id)\n",
    "\n",
    "    return image_paths, class_labels\n",
    "\n",
    "train_image_paths, train_labels = get_image_paths(train_img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad2c6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sample_images(n, cols):\n",
    "    total = len(train_image_paths)\n",
    "    num_images = min(n*cols, total)\n",
    "\n",
    "    if num_images >= total:\n",
    "        print(\"WARNING: Total images exceed available images. Returning all available images. This may take a while.\")\n",
    "\n",
    "    if (n, cols) >= (100, 5):\n",
    "        print(\"WARNING: You are attempting to plot many images. This may take a while.\")\n",
    "\n",
    "    plt.figure(figsize=(cols*3, n*3))\n",
    "\n",
    "    for i, k in enumerate(random.sample(range(total), num_images), start=1):\n",
    "        img = plt.imread(train_image_paths[k])\n",
    "        plt.subplot(n, cols, i)\n",
    "        plt.imshow(img)\n",
    "        plt.title(class_names[int(train_labels[k])])\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42ed128",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_sample_images(6,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98da872",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sample_images_per_class():\n",
    "    for c in sorted(train_df['ClassId'].unique()):\n",
    "        folder = os.path.join(train_img_path, str(c))\n",
    "        filenames = os.listdir(folder)\n",
    "        \n",
    "        sample_files = random.sample(filenames, min(5, len(filenames)))\n",
    "        \n",
    "        # Plot images\n",
    "        plt.figure(figsize=(15, 3))\n",
    "        class_name = class_names.get(c, f\"Class {c}\")\n",
    "        plt.suptitle(f\"{class_name} (Class {c})\", fontsize=16)\n",
    "        \n",
    "        for i, f in enumerate(sample_files):\n",
    "            img_path = os.path.join(folder, f)\n",
    "            img = Image.open(img_path)\n",
    "            \n",
    "            plt.subplot(1, 5, i+1)\n",
    "            plt.imshow(img)\n",
    "            plt.axis(\"off\")\n",
    "        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1414115",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_sample_images_per_class()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470cc07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolutions(image_paths, max_images=None):\n",
    "    resolutions = []\n",
    "    for path in image_paths[:max_images]:\n",
    "        with Image.open(path) as img:\n",
    "            resolutions.append(img.size)\n",
    "    return resolutions\n",
    "\n",
    "resolutions(train_image_paths, max_images=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d49b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reso_counter(image_paths, max_images=None):\n",
    "    res_counter = Counter(resolutions(image_paths, max_images))\n",
    "    print(\"Top resolutions (Width x Height : Count):\")\n",
    "    for (w, h), count in res_counter.most_common(10):\n",
    "        print(f\"{w} x {h} : {count}\")\n",
    "\n",
    "reso_counter(train_image_paths, max_images=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55268aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reso_histogram(image_paths, max_images=None):\n",
    "    widths, heights = zip(*resolutions(image_paths, max_images))\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    ax1.hist(widths, bins=20, color=\"#5F98E2\")\n",
    "    ax1.set(xlabel=\"Width (pixels)\", ylabel=\"Count\", title=\"Image widths\")\n",
    "    ax2.hist(heights, bins=20, color=\"#5F98E2\")\n",
    "    ax2.set(xlabel=\"Height (pixels)\", ylabel=\"Count\", title=\"Image heights\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return widths, heights\n",
    "\n",
    "widths, heights = reso_histogram(train_image_paths, max_images=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becc9df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.array(Image.open(train_image_paths[0])) \n",
    "plt.hist(img.ravel(), bins=255)\n",
    "plt.title(\"Pixel intensity distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f191fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_pixel_values():\n",
    "    random.seed(42)\n",
    "    pixel_values = []\n",
    "\n",
    "    # take up to 10 random images from each class\n",
    "    for c in sorted(train_df['ClassId'].unique()):\n",
    "        folder = os.path.join(train_img_path, str(c))\n",
    "        filenames = os.listdir(folder)\n",
    "        sample_files = random.sample(filenames, min(10, len(filenames)))\n",
    "        \n",
    "        for f in sample_files:\n",
    "            img = Image.open(os.path.join(folder, f)).convert('RGB')\n",
    "            img_array = np.array(img)\n",
    "            pixel_values.extend(img_array.flatten())\n",
    "\n",
    "    return pixel_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ff7f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogram\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.hist(random_pixel_values(), bins=50, color='gray')\n",
    "plt.title(\"Pixel Intensity Distribution Across Dataset\")\n",
    "plt.xlabel(\"Pixel Value (0-255)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa63d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "pixels = np.array(random_pixel_values())\n",
    "\n",
    "# Compute more statistics\n",
    "print(\"Min pixel value:\", np.min(pixels))\n",
    "print(\"25th percentile:\", np.percentile(pixels, 25))\n",
    "print(\"Median (50th percentile):\", np.median(pixels))\n",
    "print(\"75th percentile:\", np.percentile(pixels, 75))\n",
    "print(\"Max pixel value:\", np.max(pixels))\n",
    "print(\"Mean pixel value:\", np.mean(pixels))\n",
    "print(\"Standard deviation:\", np.std(pixels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fdc6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(123)\n",
    "np.random.seed(123)\n",
    "\n",
    "def augment_image(img):\n",
    "    # Rotation ±10°\n",
    "    angle = random.uniform(-10, 10)\n",
    "    img = img.rotate(angle)\n",
    "\n",
    "    # Random horizontal and vertical shift (10% max)\n",
    "    max_shift_x = int(0.1 * img.width)\n",
    "    max_shift_y = int(0.1 * img.height)\n",
    "    shift_x = random.randint(-max_shift_x, max_shift_x)\n",
    "    shift_y = random.randint(-max_shift_y, max_shift_y)\n",
    "    img = img.transform(img.size, Image.AFFINE, (1, 0, shift_x, 0, 1, shift_y))\n",
    "\n",
    "    # Random zoom/crop ±10%\n",
    "    zoom_factor = random.uniform(0.9, 1.1)\n",
    "    w, h = img.size\n",
    "    new_w, new_h = int(w * zoom_factor), int(h * zoom_factor)\n",
    "    img = img.resize((new_w, new_h), Image.BILINEAR)\n",
    "    img = img.crop((0, 0, w, h))  # crop or pad back to original size\n",
    "\n",
    "    # Brightness adjustment ±20%\n",
    "    enhancer = ImageEnhance.Brightness(img)\n",
    "    factor = random.uniform(0.8, 1.2)\n",
    "    img = enhancer.enhance(factor)\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2562fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_number = random.randint(0, 42)\n",
    "\n",
    "img1 = random_image_generator(image_number)\n",
    "\n",
    "aug_img1 = augment_image(img1)\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(img1)\n",
    "plt.title(\"Original\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(aug_img1)\n",
    "plt.title(\"Augmented\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f85506",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = sorted(os.listdir(train_img_path))\n",
    "print(\"Number of classes:\", len(classes))\n",
    "\n",
    "image_count = {}\n",
    "half_dataset = []  # final list of selected images\n",
    "\n",
    "for c in classes:\n",
    "    folder = os.path.join(train_img_path, c)\n",
    "    files = os.listdir(folder)\n",
    "\n",
    "    # count images like your EDA\n",
    "    image_count[c] = len(files)\n",
    "\n",
    "    # take 50% of the images in this class\n",
    "    half = len(files) // 2\n",
    "    selected = np.random.choice(files, half, replace=False)\n",
    "\n",
    "    # store full paths\n",
    "    for f in selected:\n",
    "        half_dataset.append(os.path.join(folder, f))\n",
    "\n",
    "print(\"Original total images:\", sum(image_count.values()))\n",
    "print(\"Reduced total images:\", len(half_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3902065f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class FilepathDataset(Dataset):\n",
    "    def __init__(self, filepaths, transform=None):\n",
    "        self.filepaths = filepaths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filepaths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.filepaths[idx]\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "\n",
    "        # label = folder name → class index\n",
    "        label = int(os.path.basename(os.path.dirname(path)))\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        else:\n",
    "            from torchvision import transforms\n",
    "            img = transforms.ToTensor()(img)\n",
    "\n",
    "        return img, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3902065f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.models import EfficientNet_B0_Weights\n",
    "IMG_SIZE = 224\n",
    "imagenet_mean = [0.485, 0.456, 0.406]\n",
    "imagenet_std  = [0.229, 0.224, 0.225]\n",
    "\n",
    "#train transform and this includes Nevas EDA findings \n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # resize all images to 224×224\n",
    "    transforms.RandomRotation(20),\n",
    "    transforms.RandomAffine(0, translate=(0.1, 0.1)),\n",
    "    transforms.ColorJitter(brightness=0.2), \n",
    "    transforms.ToTensor(),           # convert PIL image -> tensor\n",
    "    transforms.Normalize(imagenet_mean, imagenet_std)\n",
    "])\n",
    "\n",
    "#validation test transform (no augmentation)\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(imagenet_mean, imagenet_std)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a33c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 123\n",
    "full_ds = FilepathDataset(half_dataset)\n",
    "\n",
    "train_size = int(0.8 * len(half_dataset))\n",
    "val_size = len(full_ds) - train_size\n",
    "\n",
    "train_ds, val_ds = torch.utils.data.random_split(\n",
    "    full_ds,\n",
    "    [train_size, val_size],\n",
    "    generator=torch.Generator().manual_seed(SEED)\n",
    ")\n",
    "train_ds.dataset.transform = train_transforms\n",
    "val_ds.dataset.transform = val_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2850693",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader \n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6fab0d",
   "metadata": {},
   "source": [
    "See that train dataset has been reduced to 80% of inital length, as we apportioned the remaining 20% to validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59100b0",
   "metadata": {},
   "source": [
    "Will follow up with evaluation, importing Neva and Gracie's Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d056f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()\n",
    "os.chdir(\"C:/Users/billy/DataScienceToolbox-Project2\")\n",
    "print(os.listdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486083b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import efficientnet_b0, EfficientNet_B0_Weights\n",
    "\n",
    "NUM_CLASSES = 43\n",
    "\n",
    "# Load EfficientNet base model\n",
    "weights = EfficientNet_B0_Weights.IMAGENET1K_V1\n",
    "base_model = efficientnet_b0(weights=weights)\n",
    "\n",
    "# Remove classifier → keep global avg pooling only\n",
    "base_model.classifier = nn.Identity()\n",
    "\n",
    "# Freeze base model\n",
    "for param in base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Build the same custom head Neva used\n",
    "model = nn.Sequential(\n",
    "    base_model,                # (0)\n",
    "    nn.Linear(1280, 256),      # (2)\n",
    "    nn.ReLU(),                 # (3)\n",
    "    nn.Dropout(0.4),           # (4)\n",
    "    nn.Linear(256, NUM_CLASSES),  # (5)\n",
    "    nn.Softmax(dim=1)          # (6)\n",
    ")\n",
    "\n",
    "# Load the saved weights\n",
    "state_dict = torch.load(\"Neva/efficientnet_best.pth\", map_location=\"cpu\")\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d607a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import ResNet50_Weights\n",
    "from torchvision import models\n",
    "\n",
    "model_resnet18 = models.resnet18(weights=None)\n",
    "model_resnet18.fc = torch.nn.Linear(512, 43)   # example for 43 traffic signs\n",
    "state_dict_resnet18 = torch.load(\"Gracie/resnet18_traffic_signs.pth\", map_location=\"cpu\")\n",
    "model_resnet18.load_state_dict(state_dict_resnet18)\n",
    "model_resnet18.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d1e244",
   "metadata": {},
   "source": [
    "below is validation test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6370410e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model_resnet18.to(device)\n",
    "\n",
    "correct = 0\n",
    "correct_resnet18 = 0\n",
    "total = 0\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()  # only if you want loss\n",
    "\n",
    "model.eval()\n",
    "model_resnet18.eval()\n",
    "with torch.no_grad():  # disables gradient calculation\n",
    "    for images, labels in val_loader:   # your validation DataLoader\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        outputs_resnet18 = model_resnet18(images)\n",
    "        _, predicted_resnet18 = torch.max(outputs_resnet18, 1)\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        correct_resnet18 += (predicted_resnet18 == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "accuracy_resnet18 = 100 * correct_resnet18 / total\n",
    "print(f\"Validation Accuracy for Efficient Net: {accuracy:.2f}%\")\n",
    "print(f\"Validation Accuracy for ResNet18: {accuracy_resnet18:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e5ab23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [class_names[i] for i in range(NUM_CLASSES)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcda71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def classification_report_model(model, val_loader, device):\n",
    "    model.to(device)    \n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            outputs = model(images)\n",
    "            preds = outputs.argmax(1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    report = classification_report(all_labels, all_preds, zero_division=0)\n",
    "    print(report)\n",
    "    return all_labels, all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1def3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "true_effnet, pred_effnet = classification_report_model(model, val_loader, device)\n",
    "cm = confusion_matrix(true_effnet, pred_effnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09774465",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_resnet, pred_resnet = classification_report_model(model_resnet18, val_loader, device)\n",
    "cm_resnet = confusion_matrix(true_resnet, pred_resnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb84045a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,10))\n",
    "sns.heatmap(cm, annot=False, cmap=\"Blues\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix for Efficient Net Model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa3788b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,10))\n",
    "sns.heatmap(cm_resnet, annot=False, cmap=\"Blues\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix for ResNet18 Model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a5e200",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_misclassified(model, loader, device, class_names, max_images=16):\n",
    "    model.eval()\n",
    "    images_list, true_list, pred_list = [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            _, preds = outputs.max(1)\n",
    "\n",
    "            mismatch = preds != labels\n",
    "            if mismatch.any():\n",
    "                for img, t, p, m in zip(images, labels, preds, mismatch):\n",
    "                    if m:\n",
    "                        images_list.append(img.cpu())\n",
    "                        true_list.append(t.item())\n",
    "                        pred_list.append(p.item())\n",
    "                    if len(images_list) >= max_images:\n",
    "                        break\n",
    "            if len(images_list) >= max_images:\n",
    "                break\n",
    "\n",
    "    \n",
    "    n = len(images_list)\n",
    "    cols = 4\n",
    "    rows = (n + cols - 1) // cols\n",
    "    plt.figure(figsize=(3*cols, 3*rows))\n",
    "\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "    std  = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "\n",
    "    for i in range(n):\n",
    "        img = images_list[i]        \n",
    "        img = img * std + mean       \n",
    "        img = img.permute(1, 2, 0)\n",
    "        plt.subplot(rows, cols, i + 1)\n",
    "        plt.imshow(img)\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(f\"T: {class_names[true_list[i]]}\\nP: {class_names[pred_list[i]]}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# lets test it\n",
    "show_misclassified(model_resnet18, val_loader, device, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc486540",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_misclassified(model, val_loader, device, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0d0868",
   "metadata": {},
   "source": [
    "Took commands from Neva and Gracie's and attempted to configure them for both models. All of this evaluation so far has been on the validation set and has allowed us to gain a good perspective on how these models should perform on the full dataset. We now test the models on the unseen data. See eval file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
