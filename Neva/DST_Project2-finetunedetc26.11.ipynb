{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c10027f-3697-4096-bd7f-de7443c9866f",
   "metadata": {},
   "source": [
    "# Fitting EfficientNet-B0 Model"
   ]
  },
  {
   "cell_type": "code",
   "id": "4725b600-7ab8-4a15-b619-cd87d7dca57a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T10:58:19.769786Z",
     "start_time": "2025-12-02T10:58:12.938394Z"
    }
   },
   "source": [
    "from fontTools.ufoLib.glifLib import layerInfoVersion3ValueData\n",
    "from pandas.io.sas.sas_constants import dataset_length\n",
    "\n",
    "!python -m pip install pandas numpy matplotlib seaborn tqdm"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\nevaf\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\nevaf\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (2.3.5)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\nevaf\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (3.10.7)\n",
      "Requirement already satisfied: seaborn in c:\\users\\nevaf\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\nevaf\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\nevaf\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\nevaf\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\nevaf\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\nevaf\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\nevaf\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\nevaf\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\nevaf\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\nevaf\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\nevaf\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from matplotlib) (12.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\nevaf\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\nevaf\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\nevaf\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "cell_type": "markdown",
   "id": "6661851d-62e1-4b6c-86ee-b37bea7a27e0",
   "metadata": {},
   "source": [
    "The below specifies the packages we need to download"
   ]
  },
  {
   "cell_type": "code",
   "id": "1ece214b-388c-4fa9-9852-ee8bf7ae94b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T09:49:47.619285Z",
     "start_time": "2025-12-02T09:49:47.588348Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import os\n",
    "from PIL import Image, ImageEnhance"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This chunk extracts the data from the zip file, and saves it into a \"raw data\" file on my laptop.",
   "id": "ea663eeb-8030-4809-bdab-002d0c211e6f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T10:59:08.452004Z",
     "start_time": "2025-12-02T10:59:08.443379Z"
    }
   },
   "cell_type": "code",
   "source": [
    "os.chdir(\"C:/Projects/DST_Project2\")\n",
    "print(\"Current directory:\", os.getcwd())"
   ],
   "id": "c9a0c581c251e61d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: C:\\Projects\\DST_Project2\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T01:08:18.964369Z",
     "start_time": "2025-12-02T01:06:41.994367Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import zipfile\n",
    "\n",
    "# 1. Change working directory\n",
    "os.chdir(\"C:/Projects/DST_Project2\")\n",
    "print(\"Current directory:\", os.getcwd())\n",
    "\n",
    "# 2. Path to ZIP file\n",
    "zip_path = \"C:/Users/Nevaf/PycharmProjects/DataScienceToolbox-Project2/Neva/traffic_raw.zip\"  # <-- full path including .zip\n",
    "\n",
    "# 3. Extract ZIP\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"C:/Neva/data\")  # Extract to folder\n",
    "print(\"ZIP extracted successfully!\")"
   ],
   "id": "41e5cf87-f2cb-4608-aac4-b78682a8c5a6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: C:\\Users\\Nevaf\\PycharmProjects\\DataScienceToolbox-Project2\\Neva\n",
      "ZIP extracted successfully!\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T23:52:29.436832Z",
     "start_time": "2025-12-01T23:52:29.430990Z"
    }
   },
   "cell_type": "markdown",
   "source": "# EfficientNet-B0 Model Fitting",
   "id": "8a76deb7e283795f"
  },
  {
   "cell_type": "markdown",
   "id": "d7fef2c8-a50d-4f59-971c-a91eeec4e5dc",
   "metadata": {},
   "source": [
    "STEP 1: INSTALL DEPENDENCIES"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"\"EfficientNet-B0 model fitting and fine tuning.\n",
    "\n",
    "Here we implement transfer learning using EfficientNet-B0 for classifying traffic signs from the German Traffic Sign Recognition Benchmark (GTSRB)\n",
    "\n",
    "The following script shows:\n",
    "- Dataset preparation\n",
    "- Random 50% class-balanced downsampling\n",
    "- Data augmentation and preprocessing\n",
    "- Transfer learning with pretrained model EfficientNet-B0\n",
    "- Fine-tuning of final layers\n",
    "- Visualisation of model accuracy\n",
    "\n",
    "The code is annotated with markdown and NumPy docstrings\n",
    "\"\"\""
   ],
   "id": "17a83d4dc594f431",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fd826f31-0752-4ccd-a01b-d38c1dc1c1a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T10:59:54.604980Z",
     "start_time": "2025-12-02T10:59:54.581310Z"
    }
   },
   "source": [
    "import os\n",
    "import copy \n",
    "import numpy as np\n",
    "from typing import Tuple, Dict, List\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset , random_split\n",
    "from torchvision import transforms\n",
    "from torchvision.models import efficientnet_b0, EfficientNet_B0_Weights\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm import tqdm #installs a progress bar for epochs\n"
   ],
   "outputs": [],
   "execution_count": 39
  },
  {
   "cell_type": "markdown",
   "id": "c360c2bb-c8c5-4f4e-a3a4-3d4870dc8eeb",
   "metadata": {},
   "source": "We've installed and imported all necessary libraries and functions."
  },
  {
   "cell_type": "code",
   "id": "ad68450e-d0f7-47ca-a6d9-e6409615fa61",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T11:00:03.609922Z",
     "start_time": "2025-12-02T11:00:03.551162Z"
    }
   },
   "source": [
    "class Config:\n",
    "    \"\"\"Configuration parameters for model training.\"\"\"\n",
    "\n",
    "    # Device configuration\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    IMG_SIZE = 224 # necessary for EfficientNet\n",
    "    BATCH_SIZE = 32\n",
    "    NUM_CLASSES = 43\n",
    "    EPOCHS = 10\n",
    "    FINE_TUNE_EPOCHS = 5\n",
    "    LEARNING_RATE = 1e-3\n",
    "    FINE_TUNE_LR = 1e-4\n",
    "\n",
    "    SEED = 42\n",
    "    TRAIN_SPLIT = 0.8\n",
    "    PATIENCE = 5\n",
    "    torch.manual_seed(SEED)\n",
    "\n",
    "    #Paths\n",
    "    DATA_DIR = Path(\"data/raw/Train\")\n",
    "    MODEL_SAVE_PATH = Path(\"models/efficientnet_best.pth\")"
   ],
   "outputs": [],
   "execution_count": 40
  },
  {
   "cell_type": "markdown",
   "id": "19827e3a-2d7c-4083-acdc-177f5c453645",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": "Prepare the dataset after EDA. We select 50% of the images per class to fit a model to, and we create a custom dataset class."
  },
  {
   "cell_type": "code",
   "id": "861be2f2-e078-4e1a-877e-a4019c2da28e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T11:00:16.596539Z",
     "start_time": "2025-12-02T11:00:16.590066Z"
    }
   },
   "source": "print(os.getcwd())  # confirms current working directory\n",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Projects\\DST_Project2\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "id": "6a05f4d9-b0b0-451a-82bf-006ed6b390ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T11:00:19.495377Z",
     "start_time": "2025-12-02T11:00:19.467864Z"
    }
   },
   "source": [
    "class TrafficSignDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset for our model.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    filepaths : List[str]\n",
    "    List of paths to image files.\n",
    "    transform : torchvision.transforms.Compose\n",
    "    Image transformations applied to the images.\n",
    "\n",
    "    Attributes:\n",
    "    ------------\n",
    "    filepaths : List[str]\n",
    "    List of paths to image files.\n",
    "    transform : torchvision.transforms.Compose\n",
    "    Image transformations applied to the images.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, filepaths: List[str], transform=None):\n",
    "        self.filepaths = filepaths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.filepaths)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, int] :\n",
    "        \"\"\"\n",
    "    Take a single sample from the dataset.\n",
    "\n",
    "    Parameters:\n",
    "    ------------\n",
    "    idx : int\n",
    "    Index of the sample.\n",
    "\n",
    "    Returns:\n",
    "    ----------\n",
    "    Tuple[torch.Tensor, int]\n",
    "    The image tensor and its corresponding class index.\n",
    "    \"\"\"\n",
    "        path = self.filepaths[idx]\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        # Extract label from the directory structure\n",
    "        label = int(os.path.basename(os.path.dirname(path)))\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        else:\n",
    "            # Use default transformation\n",
    "            img = transforms.ToTensor()(img)\n",
    "\n",
    "        return img, label\n"
   ],
   "outputs": [],
   "execution_count": 42
  },
  {
   "cell_type": "markdown",
   "id": "481abf2d-27d3-4b4d-b12c-374f9bc09e13",
   "metadata": {},
   "source": [
    "Now we apply transformations/ perform data augmentation. This dataset is already quite varied but"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T11:00:31.213576Z",
     "start_time": "2025-12-02T11:00:31.175779Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def prepare_data(config: Config) -> Tuple[DataLoader, DataLoader]:\n",
    "    \"\"\"\n",
    "    Prepare the train and validation data loaders.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    config : Config\n",
    "        Configuration object with hyperparameters\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[DataLoader, DataLoader]\n",
    "        Training and validation data loaders\n",
    "    \"\"\"\n",
    "    # Set random seed for reproducibility\n",
    "    torch.manual_seed(config.SEED)\n",
    "    np.random.seed(config.SEED)\n",
    "\n",
    "    # Collect all image paths\n",
    "    all_filepaths = []\n",
    "    classes = sorted([d for d in os.listdir(config.DATA_DIR)\n",
    "                     if os.path.isdir(config.DATA_DIR / d)])\n",
    "\n",
    "    for class_id in classes:\n",
    "        class_dir = config.DATA_DIR / class_id\n",
    "        files = [str(class_dir / f) for f in os.listdir(class_dir)\n",
    "                if f.lower().endswith(('.png', '.jpg', '.jpeg', '.ppm'))]\n",
    "\n",
    "        # Sample 50% of images per class\n",
    "        sample_size = len(files) // 2\n",
    "        sampled_files = np.random.choice(files, sample_size, replace=False)\n",
    "        all_filepaths.extend(sampled_files)\n",
    "\n",
    "    print(f\"Total images after sampling: {len(all_filepaths)}\")\n",
    "\n",
    "    # Define transforms\n",
    "    train_transforms = transforms.Compose([\n",
    "        transforms.Resize((config.IMG_SIZE, config.IMG_SIZE)),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.RandomAffine(0, translate=(0.1, 0.1)),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                           std=[0.229, 0.224, 0.225])  # ImageNet stats\n",
    "    ])\n",
    "\n",
    "    val_transforms = transforms.Compose([\n",
    "        transforms.Resize((config.IMG_SIZE, config.IMG_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                           std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    # Create dataset and split\n",
    "    full_dataset = TrafficSignDataset(all_filepaths)\n",
    "\n",
    "    train_size = int(config.TRAIN_SPLIT * len(full_dataset))\n",
    "    val_size = len(full_dataset) - train_size\n",
    "\n",
    "    train_dataset, val_dataset = random_split(\n",
    "        full_dataset,\n",
    "        [train_size, val_size],\n",
    "        generator=torch.Generator().manual_seed(config.SEED)\n",
    "    )\n",
    "\n",
    "    # Apply transforms\n",
    "    train_dataset.dataset.transform = train_transforms\n",
    "    val_dataset.dataset.transform = val_transforms\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config.BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=0,  # Set to 0 for Windows compatibility\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config.BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader\n"
   ],
   "id": "c78c51638fe2e262",
   "outputs": [],
   "execution_count": 43
  },
  {
   "cell_type": "markdown",
   "id": "2269f7c9-c8fc-49c6-ba92-2fedb51ff31a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "Now we start setting up the model.\n",
    "Remove classifier â†’ keep global average pooling\n",
    "reeze base model\n",
    "Your custom classifier head\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T01:50:50.162886Z",
     "start_time": "2025-12-02T01:50:50.149354Z"
    }
   },
   "cell_type": "markdown",
   "source": "# Model Building",
   "id": "979c1fa707f20fee"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T11:00:37.715390Z",
     "start_time": "2025-12-02T11:00:37.701701Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def build_model(config: Config, freeze_base: bool = True) -> nn.Module:\n",
    "    \"\"\"\n",
    "    Build EfficientNet-B0 model with custom classifier.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    config : Config\n",
    "        Configuration object\n",
    "    freeze_base : bool, default=True\n",
    "        Whether to freeze the base model weights\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    nn.Module\n",
    "        Complete model ready for training\n",
    "    \"\"\"\n",
    "    # Load pre-trained EfficientNet\n",
    "    weights = EfficientNet_B0_Weights.IMAGENET1K_V1\n",
    "    base_model = efficientnet_b0(weights=weights)\n",
    "\n",
    "    # Remove original classifier\n",
    "    base_model.classifier = nn.Identity()\n",
    "\n",
    "    # Freeze base model if specified\n",
    "    if freeze_base:\n",
    "        for param in base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    # Build complete model with custom classifier\n",
    "    model = nn.Sequential(\n",
    "        base_model,\n",
    "        nn.Linear(1280, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.4),\n",
    "        nn.Linear(256, config.NUM_CLASSES)\n",
    "    )\n",
    "\n",
    "    return model.to(config.DEVICE)\n"
   ],
   "id": "706edece3154c66e",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T16:27:06.289578Z",
     "start_time": "2025-12-02T16:27:06.123653Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_epoch(model: nn.Module,\n",
    "                train_loader: DataLoader,\n",
    "                optimizer: torch.optim.Optimizer,\n",
    "                device: torch.device,\n",
    "                criterion: nn.Module = nn.CrossEntropyLoss()) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Train model for one epoch.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : nn.Module\n",
    "        Model to train\n",
    "    train_loader : DataLoader\n",
    "        Training data loader\n",
    "    criterion : nn.Module\n",
    "        Loss function\n",
    "    optimizer : torch.optim.Optimizer\n",
    "        Optimizer\n",
    "    device : torch.device\n",
    "        Device to train on\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[float, float]\n",
    "        Average loss and accuracy for the epoch\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "\n",
    "    for images, labels in tqdm(train_loader, desc=\"Training\"):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        running_corrects += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    epoch_acc = running_corrects / len(train_loader.dataset)\n",
    "\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "def validate(model: nn.Module,\n",
    "            val_loader: DataLoader,\n",
    "            device: torch.device,\n",
    "            criterion: nn.Module = nn.CrossEntropyLoss()) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Validate model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : nn.Module\n",
    "        Model to validate\n",
    "    val_loader : DataLoader\n",
    "        Validation data loader\n",
    "    criterion : nn.Module\n",
    "        Loss function\n",
    "    device : torch.device\n",
    "        Device to run validation on\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[float, float]\n",
    "        Validation loss and accuracy\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            running_corrects += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "    val_loss = running_loss / len(val_loader.dataset)\n",
    "    val_acc = running_corrects / len(val_loader.dataset)\n",
    "\n",
    "    return val_loss, val_acc\n",
    "\n",
    "\n",
    "def train_model(model: nn.Module,\n",
    "               train_loader: DataLoader,\n",
    "               val_loader: DataLoader,\n",
    "               config: Config,\n",
    "               optimizer: torch.optim.Optimizer = None) -> Dict:\n",
    "    \"\"\"\n",
    "    Complete training loop with early stopping.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : nn.Module\n",
    "        Model to train\n",
    "    train_loader : DataLoader\n",
    "        Training data loader\n",
    "    val_loader : DataLoader\n",
    "        Validation data loader\n",
    "    config : Config\n",
    "        Configuration object\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dict\n",
    "        Training history with losses and accuracies\n",
    "    \"\"\"\n",
    "    device = config.DEVICE\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    if optimizer is None:\n",
    "        optimizer = Adam(model.parameters(), lr=config.LEARNING_RATE)\n",
    "\n",
    "    scheduler = ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode='min',\n",
    "        factor=0.2,\n",
    "        patience=3,\n",
    "        min_lr=1e-6\n",
    "    )\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    patience_counter = 0\n",
    "    history = {'train_loss': [], 'train_acc': [],\n",
    "               'val_loss': [], 'val_acc': []}\n",
    "\n",
    "    for epoch in range(Config.EPOCHS):\n",
    "        print(f\"\\nEpoch {epoch+1}/{config.EPOCHS}\")\n",
    "\n",
    "        # Train\n",
    "        train_loss, train_acc = train_epoch(\n",
    "            model, train_loader, optimizer, config.DEVICE, criterion\n",
    "        )\n",
    "\n",
    "        # Validate\n",
    "        val_loss, val_acc = validate(\n",
    "            model, val_loader, config.DEVICE, criterion\n",
    "        )\n",
    "\n",
    "        # Update scheduler\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        # Store history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "\n",
    "        print(f\"Train Loss: {train_loss:.4f} Acc: {train_acc:.4f}\")\n",
    "        print(f\"Val   Loss: {val_loss:.4f} Acc: {val_acc:.4f}\")\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            patience_counter = 0\n",
    "\n",
    "            # Save best model\n",
    "            config.MODEL_SAVE_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "            torch.save(best_model_state, config.MODEL_SAVE_PATH)\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= config.PATIENCE:\n",
    "                print(f\"\\nEarly stopping triggered after {epoch+1} epochs\")\n",
    "                break\n",
    "\n",
    "    # Load best model\n",
    "    model.load_state_dict(best_model_state)\n",
    "    return history\n"
   ],
   "id": "e66e55c74e5bb066",
   "outputs": [],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T11:24:47.333266Z",
     "start_time": "2025-12-02T11:24:40.463279Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def main():\n",
    "    \"\"\"Main execution function.\"\"\"\n",
    "    # Initialize configuration\n",
    "    config = Config()\n",
    "\n",
    "    print(f\"Using device: {config.DEVICE}\")\n",
    "    print(f\"Number of classes: {config.NUM_CLASSES}\")\n",
    "\n",
    "    # Prepare data\n",
    "    print(\"\\nPreparing data...\")\n",
    "    train_loader, val_loader = prepare_data(config)\n",
    "\n",
    "    # Build model\n",
    "    print(\"\\nBuilding model...\")\n",
    "    model = build_model(config, freeze_base=True)\n",
    "\n",
    "    # Train model\n",
    "    print(\"\\nStarting training...\")\n",
    "    history = train_model(model, train_loader, val_loader, config)\n",
    "\n",
    "    print(\"\\nTraining complete!\")\n",
    "    print(f\"Best model saved to: {config.MODEL_SAVE_PATH}\")\n",
    "\n",
    "    return model, history\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model, history = main()"
   ],
   "id": "ccae099beff4cad6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Number of classes: 43\n",
      "\n",
      "Preparing data...\n",
      "Total images after sampling: 19604\n",
      "\n",
      "Building model...\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/491 [00:04<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[60]\u001B[39m\u001B[32m, line 28\u001B[39m\n\u001B[32m     24\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m model, history\n\u001B[32m     27\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[34m__name__\u001B[39m == \u001B[33m\"\u001B[39m\u001B[33m__main__\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m---> \u001B[39m\u001B[32m28\u001B[39m     model, history = \u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[60]\u001B[39m\u001B[32m, line 19\u001B[39m, in \u001B[36mmain\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m     17\u001B[39m \u001B[38;5;66;03m# Train model\u001B[39;00m\n\u001B[32m     18\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33mStarting training...\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m19\u001B[39m history = \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     21\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33mTraining complete!\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     22\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mBest model saved to: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig.MODEL_SAVE_PATH\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[59]\u001B[39m\u001B[32m, line 141\u001B[39m, in \u001B[36mtrain_model\u001B[39m\u001B[34m(model, train_loader, val_loader, config, optimizer)\u001B[39m\n\u001B[32m    138\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch+\u001B[32m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig.EPOCHS\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m    140\u001B[39m \u001B[38;5;66;03m# Train\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m141\u001B[39m train_loss, train_acc = \u001B[43mtrain_epoch\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    142\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m.\u001B[49m\u001B[43mDEVICE\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\n\u001B[32m    143\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    145\u001B[39m \u001B[38;5;66;03m# Validate\u001B[39;00m\n\u001B[32m    146\u001B[39m val_loss, val_acc = validate(\n\u001B[32m    147\u001B[39m     model, val_loader, criterion, config.DEVICE\n\u001B[32m    148\u001B[39m )\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[59]\u001B[39m\u001B[32m, line 37\u001B[39m, in \u001B[36mtrain_epoch\u001B[39m\u001B[34m(model, train_loader, optimizer, device, criterion)\u001B[39m\n\u001B[32m     33\u001B[39m labels = labels.to(device)\n\u001B[32m     35\u001B[39m optimizer.zero_grad()\n\u001B[32m---> \u001B[39m\u001B[32m37\u001B[39m outputs = \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimages\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     38\u001B[39m loss = criterion(outputs, labels)\n\u001B[32m     40\u001B[39m loss.backward()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1774\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1775\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1781\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1782\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1783\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1784\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1785\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1786\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1788\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1789\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001B[39m, in \u001B[36mSequential.forward\u001B[39m\u001B[34m(self, input)\u001B[39m\n\u001B[32m    246\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    247\u001B[39m \u001B[33;03mRuns the forward pass.\u001B[39;00m\n\u001B[32m    248\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    249\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m250\u001B[39m     \u001B[38;5;28minput\u001B[39m = \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m    251\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1774\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1775\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1781\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1782\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1783\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1784\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1785\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1786\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1788\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1789\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\torchvision\\models\\efficientnet.py:344\u001B[39m, in \u001B[36mEfficientNet.forward\u001B[39m\u001B[34m(self, x)\u001B[39m\n\u001B[32m    343\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x: Tensor) -> Tensor:\n\u001B[32m--> \u001B[39m\u001B[32m344\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_forward_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\torchvision\\models\\efficientnet.py:334\u001B[39m, in \u001B[36mEfficientNet._forward_impl\u001B[39m\u001B[34m(self, x)\u001B[39m\n\u001B[32m    333\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_forward_impl\u001B[39m(\u001B[38;5;28mself\u001B[39m, x: Tensor) -> Tensor:\n\u001B[32m--> \u001B[39m\u001B[32m334\u001B[39m     x = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mfeatures\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    336\u001B[39m     x = \u001B[38;5;28mself\u001B[39m.avgpool(x)\n\u001B[32m    337\u001B[39m     x = torch.flatten(x, \u001B[32m1\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1774\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1775\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1781\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1782\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1783\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1784\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1785\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1786\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1788\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1789\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001B[39m, in \u001B[36mSequential.forward\u001B[39m\u001B[34m(self, input)\u001B[39m\n\u001B[32m    246\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    247\u001B[39m \u001B[33;03mRuns the forward pass.\u001B[39;00m\n\u001B[32m    248\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    249\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m250\u001B[39m     \u001B[38;5;28minput\u001B[39m = \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m    251\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1774\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1775\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1781\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1782\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1783\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1784\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1785\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1786\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1788\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1789\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001B[39m, in \u001B[36mSequential.forward\u001B[39m\u001B[34m(self, input)\u001B[39m\n\u001B[32m    246\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    247\u001B[39m \u001B[33;03mRuns the forward pass.\u001B[39;00m\n\u001B[32m    248\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    249\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m250\u001B[39m     \u001B[38;5;28minput\u001B[39m = \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m    251\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1774\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1775\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1781\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1782\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1783\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1784\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1785\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1786\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1788\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1789\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\torchvision\\models\\efficientnet.py:165\u001B[39m, in \u001B[36mMBConv.forward\u001B[39m\u001B[34m(self, input)\u001B[39m\n\u001B[32m    164\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) -> Tensor:\n\u001B[32m--> \u001B[39m\u001B[32m165\u001B[39m     result = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mblock\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m    166\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.use_res_connect:\n\u001B[32m    167\u001B[39m         result = \u001B[38;5;28mself\u001B[39m.stochastic_depth(result)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1774\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1775\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1781\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1782\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1783\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1784\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1785\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1786\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1788\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1789\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001B[39m, in \u001B[36mSequential.forward\u001B[39m\u001B[34m(self, input)\u001B[39m\n\u001B[32m    246\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    247\u001B[39m \u001B[33;03mRuns the forward pass.\u001B[39;00m\n\u001B[32m    248\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    249\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m250\u001B[39m     \u001B[38;5;28minput\u001B[39m = \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m    251\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1774\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1775\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1781\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1782\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1783\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1784\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1785\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1786\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1788\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1789\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001B[39m, in \u001B[36mSequential.forward\u001B[39m\u001B[34m(self, input)\u001B[39m\n\u001B[32m    246\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    247\u001B[39m \u001B[33;03mRuns the forward pass.\u001B[39;00m\n\u001B[32m    248\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    249\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m250\u001B[39m     \u001B[38;5;28minput\u001B[39m = \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m    251\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1774\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1775\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1781\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1782\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1783\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1784\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1785\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1786\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1788\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1789\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548\u001B[39m, in \u001B[36mConv2d.forward\u001B[39m\u001B[34m(self, input)\u001B[39m\n\u001B[32m    547\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) -> Tensor:\n\u001B[32m--> \u001B[39m\u001B[32m548\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_conv_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:543\u001B[39m, in \u001B[36mConv2d._conv_forward\u001B[39m\u001B[34m(self, input, weight, bias)\u001B[39m\n\u001B[32m    531\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.padding_mode != \u001B[33m\"\u001B[39m\u001B[33mzeros\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m    532\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m F.conv2d(\n\u001B[32m    533\u001B[39m         F.pad(\n\u001B[32m    534\u001B[39m             \u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m._reversed_padding_repeated_twice, mode=\u001B[38;5;28mself\u001B[39m.padding_mode\n\u001B[32m   (...)\u001B[39m\u001B[32m    541\u001B[39m         \u001B[38;5;28mself\u001B[39m.groups,\n\u001B[32m    542\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m543\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[43m.\u001B[49m\u001B[43mconv2d\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    544\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdilation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgroups\u001B[49m\n\u001B[32m    545\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T16:27:26.840941Z",
     "start_time": "2025-12-02T16:27:26.313265Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_model_simple(model_path='models/efficientnet_best.pth', num_classes=43):\n",
    "    \"\"\"\n",
    "    Load the saved model - simplest approach.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model_path : str\n",
    "        Path to saved model weights\n",
    "    num_classes : int\n",
    "        Number of output classes (43 for traffic signs)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    torch.nn.Module\n",
    "        Loaded model ready for inference\n",
    "    \"\"\"\n",
    "    # Recreate the exact same model architecture\n",
    "    weights = EfficientNet_B0_Weights.IMAGENET1K_V1\n",
    "    base_model = efficientnet_b0(weights=weights)\n",
    "    base_model.classifier = nn.Identity()\n",
    "\n",
    "    model = nn.Sequential(\n",
    "        base_model,\n",
    "        nn.Linear(1280, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.4),\n",
    "        nn.Linear(256, num_classes)\n",
    "    )\n",
    "\n",
    "    # Load the saved weights\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model = model.to(device)\n",
    "    model.eval()  # Set to evaluation mode\n",
    "\n",
    "    print(f\"âœ“ Model loaded from {model_path}\")\n",
    "    print(f\"âœ“ Device: {device}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "model = load_model_simple(\n",
    "    model_path=\"C:/Projects/DST_Project2/models/efficientnet_best.pth\",\n",
    "    num_classes=43\n",
    ")\n",
    "\n"
   ],
   "id": "b6910d8bb81ec2a0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Model loaded from C:/Projects/DST_Project2/models/efficientnet_best.pth\n",
      "âœ“ Device: cpu\n"
     ]
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T11:18:43.266703Z",
     "start_time": "2025-12-02T11:18:42.538683Z"
    }
   },
   "cell_type": "code",
   "source": "train_loader, val_loader = prepare_data(Config)\n",
   "id": "772a18ab1c829fbb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images after sampling: 19604\n"
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T11:31:17.270660Z",
     "start_time": "2025-12-02T11:31:17.185917Z"
    }
   },
   "cell_type": "code",
   "source": "Config.EPOCHS = 2",
   "id": "2161f9274d9e15f9",
   "outputs": [],
   "execution_count": 62
  },
  {
   "cell_type": "code",
   "id": "34e49a70-35af-46e7-be61-72f747d1043c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T16:55:41.345309Z",
     "start_time": "2025-12-02T16:27:51.433054Z"
    }
   },
   "source": [
    "def progressive_fine_tuning(model, train_loader, val_loader, Config):\n",
    "    \"\"\"\n",
    "    Implement progressive fine-tuning strategy.\n",
    "    Already trained classifier only\n",
    "    Stage 1: Unfreeze last 2 blocks + classifier\n",
    "    Stage 2: Full fine-tuning\n",
    "    \"\"\"\n",
    "\n",
    "    # Unfreeze last 2 blocks\n",
    "    print(\"Fine-tuning last 2 blocks\")\n",
    "    for name, param in model[0].named_parameters():\n",
    "        if name.startswith(\"features.6\") or name.startswith(\"features.7\"):\n",
    "            param.requires_grad = True\n",
    "\n",
    "    # Use lower learning rate\n",
    "    optimizer = Adam([\n",
    "        {'params': model[0].features[6].parameters(), 'lr': 3e-4},\n",
    "        {'params': model[0].features[7].parameters(), 'lr': 3e-4},\n",
    "        {'params': model[1].parameters(), 'lr': 1e-3},\n",
    "        {'params': model[3].parameters(), 'lr': 1e-3}\n",
    "    ])\n",
    "    history_finetune = train_model(model, train_loader, val_loader,\n",
    "                                  Config, optimizer = optimizer)\n",
    "\n",
    "    return history_finetune\n",
    "\n",
    "progressive_fine_tuning(model, train_loader, val_loader, Config)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning last 2 blocks\n",
      "\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  28%|â–ˆâ–ˆâ–Š       | 137/491 [27:47<1:11:48, 12.17s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[69]\u001B[39m\u001B[32m, line 27\u001B[39m\n\u001B[32m     22\u001B[39m     history_finetune = train_model(model, train_loader, val_loader,\n\u001B[32m     23\u001B[39m                                   Config, optimizer = optimizer)\n\u001B[32m     25\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m history_finetune\n\u001B[32m---> \u001B[39m\u001B[32m27\u001B[39m \u001B[43mprogressive_fine_tuning\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mConfig\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[69]\u001B[39m\u001B[32m, line 22\u001B[39m, in \u001B[36mprogressive_fine_tuning\u001B[39m\u001B[34m(model, train_loader, val_loader, Config)\u001B[39m\n\u001B[32m     15\u001B[39m \u001B[38;5;66;03m# Use lower learning rate\u001B[39;00m\n\u001B[32m     16\u001B[39m optimizer = Adam([\n\u001B[32m     17\u001B[39m     {\u001B[33m'\u001B[39m\u001B[33mparams\u001B[39m\u001B[33m'\u001B[39m: model[\u001B[32m0\u001B[39m].features[\u001B[32m6\u001B[39m].parameters(), \u001B[33m'\u001B[39m\u001B[33mlr\u001B[39m\u001B[33m'\u001B[39m: \u001B[32m3e-4\u001B[39m},\n\u001B[32m     18\u001B[39m     {\u001B[33m'\u001B[39m\u001B[33mparams\u001B[39m\u001B[33m'\u001B[39m: model[\u001B[32m0\u001B[39m].features[\u001B[32m7\u001B[39m].parameters(), \u001B[33m'\u001B[39m\u001B[33mlr\u001B[39m\u001B[33m'\u001B[39m: \u001B[32m3e-4\u001B[39m},\n\u001B[32m     19\u001B[39m     {\u001B[33m'\u001B[39m\u001B[33mparams\u001B[39m\u001B[33m'\u001B[39m: model[\u001B[32m1\u001B[39m].parameters(), \u001B[33m'\u001B[39m\u001B[33mlr\u001B[39m\u001B[33m'\u001B[39m: \u001B[32m1e-3\u001B[39m},\n\u001B[32m     20\u001B[39m     {\u001B[33m'\u001B[39m\u001B[33mparams\u001B[39m\u001B[33m'\u001B[39m: model[\u001B[32m3\u001B[39m].parameters(), \u001B[33m'\u001B[39m\u001B[33mlr\u001B[39m\u001B[33m'\u001B[39m: \u001B[32m1e-3\u001B[39m}\n\u001B[32m     21\u001B[39m ])\n\u001B[32m---> \u001B[39m\u001B[32m22\u001B[39m history_finetune = \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     23\u001B[39m \u001B[43m                              \u001B[49m\u001B[43mConfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     25\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m history_finetune\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[67]\u001B[39m\u001B[32m, line 145\u001B[39m, in \u001B[36mtrain_model\u001B[39m\u001B[34m(model, train_loader, val_loader, config, optimizer)\u001B[39m\n\u001B[32m    142\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch+\u001B[32m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig.EPOCHS\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m    144\u001B[39m \u001B[38;5;66;03m# Train\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m145\u001B[39m train_loss, train_acc = \u001B[43mtrain_epoch\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    146\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m.\u001B[49m\u001B[43mDEVICE\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\n\u001B[32m    147\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    149\u001B[39m \u001B[38;5;66;03m# Validate\u001B[39;00m\n\u001B[32m    150\u001B[39m val_loss, val_acc = validate(\n\u001B[32m    151\u001B[39m     model, val_loader, config.DEVICE, criterion\n\u001B[32m    152\u001B[39m )\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[67]\u001B[39m\u001B[32m, line 40\u001B[39m, in \u001B[36mtrain_epoch\u001B[39m\u001B[34m(model, train_loader, optimizer, device, criterion)\u001B[39m\n\u001B[32m     37\u001B[39m outputs = model(images)\n\u001B[32m     38\u001B[39m loss = criterion(outputs, labels)\n\u001B[32m---> \u001B[39m\u001B[32m40\u001B[39m \u001B[43mloss\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     41\u001B[39m optimizer.step()\n\u001B[32m     43\u001B[39m running_loss += loss.item() * images.size(\u001B[32m0\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\torch\\_tensor.py:625\u001B[39m, in \u001B[36mTensor.backward\u001B[39m\u001B[34m(self, gradient, retain_graph, create_graph, inputs)\u001B[39m\n\u001B[32m    615\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m    616\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[32m    617\u001B[39m         Tensor.backward,\n\u001B[32m    618\u001B[39m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[32m   (...)\u001B[39m\u001B[32m    623\u001B[39m         inputs=inputs,\n\u001B[32m    624\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m625\u001B[39m \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mautograd\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    626\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m=\u001B[49m\u001B[43minputs\u001B[49m\n\u001B[32m    627\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\torch\\autograd\\__init__.py:354\u001B[39m, in \u001B[36mbackward\u001B[39m\u001B[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[39m\n\u001B[32m    349\u001B[39m     retain_graph = create_graph\n\u001B[32m    351\u001B[39m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[32m    352\u001B[39m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[32m    353\u001B[39m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m354\u001B[39m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    355\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    356\u001B[39m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    357\u001B[39m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    358\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    359\u001B[39m \u001B[43m    \u001B[49m\u001B[43minputs_tuple\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    360\u001B[39m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    361\u001B[39m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    362\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\torch\\autograd\\graph.py:841\u001B[39m, in \u001B[36m_engine_run_backward\u001B[39m\u001B[34m(t_outputs, *args, **kwargs)\u001B[39m\n\u001B[32m    839\u001B[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[32m    840\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m841\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_execution_engine\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[32m    842\u001B[39m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\n\u001B[32m    843\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[32m    844\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    845\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 69
  },
  {
   "cell_type": "markdown",
   "id": "b9f6eec9-913b-45f3-8deb-34c3f7dc4d9a",
   "metadata": {},
   "source": [
    "#classifier = layers 1 and 4 of the Sequential\n",
    "#backbone fine-tuned layers = features.6 and features.7\n",
    "different learing rate for classifier and backbone\n",
    "       # fast learning for classifier\n",
    "\n",
    "    # slow learning for fine-tuned blocks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a818b96-40c4-4b4a-bc91-b0feaf6f5f55",
   "metadata": {},
   "source": "Fine tuning loop"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T17:09:06.239543Z",
     "start_time": "2025-12-02T17:09:06.159260Z"
    }
   },
   "cell_type": "code",
   "source": [
    "classifier_params = list(model[1].parameters()) + list(model[4].parameters())\n",
    "\n",
    "backbone_params = [\n",
    "    p for name, p in model.named_parameters()\n",
    "    if name.startswith(\"features.6\") or name.startswith(\"features.7\")\n",
    "]\n",
    "\n",
    "optimizer = torch.optim.Adam([\n",
    "    {\"params\": classifier_params, \"lr\": 1e-3},\n",
    "    {\"params\": backbone_params, \"lr\": 1e-4}\n",
    "])"
   ],
   "id": "85329768a4a4298d",
   "outputs": [],
   "execution_count": 74
  },
  {
   "cell_type": "code",
   "id": "0f8a2751-fef6-4bb2-9a83-52c7a51a212f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T18:06:11.452856Z",
     "start_time": "2025-12-02T17:09:12.245981Z"
    }
   },
   "source": [
    "for name, _ in model[0].named_parameters():\n",
    "    print(name)\n",
    "EPOCHS = 5\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
    "\n",
    "    # ---- Train ----\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "\n",
    "    for images, labels in tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # FIX: multiply by batch size\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        running_corrects += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "    # FIX: divide by dataset size (not by batches)\n",
    "    train_loss = running_loss / len(train_loader.dataset)\n",
    "    train_acc = running_corrects / len(train_loader.dataset)\n",
    "\n",
    "    # ---- Validation ----\n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    val_running_corrects = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # FIX: multiply by batch size\n",
    "            val_running_loss += loss.item() * images.size(0)\n",
    "            val_running_corrects += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "    # FIX: divide by dataset size\n",
    "    val_loss = val_running_loss / len(val_loader.dataset)\n",
    "    val_acc = val_running_corrects / len(val_loader.dataset)\n",
    "\n",
    "    # scheduler takes average val_loss\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    print(f\"Train Loss: {train_loss:.4f} Acc: {train_acc:.4f}\")\n",
    "    print(f\"Val   Loss: {val_loss:.4f} Acc: {val_acc:.4f}\")\n",
    "\n",
    "    # ---- Early stopping ----\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_state = copy.deepcopy(model.state_dict())\n",
    "        torch.save(best_model_state, \"efficientnet_best.pth\")\n",
    "        wait = 0\n",
    "    else:\n",
    "        wait += 1\n",
    "        if wait >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.0.0.weight\n",
      "features.0.1.weight\n",
      "features.0.1.bias\n",
      "features.1.0.block.0.0.weight\n",
      "features.1.0.block.0.1.weight\n",
      "features.1.0.block.0.1.bias\n",
      "features.1.0.block.1.fc1.weight\n",
      "features.1.0.block.1.fc1.bias\n",
      "features.1.0.block.1.fc2.weight\n",
      "features.1.0.block.1.fc2.bias\n",
      "features.1.0.block.2.0.weight\n",
      "features.1.0.block.2.1.weight\n",
      "features.1.0.block.2.1.bias\n",
      "features.2.0.block.0.0.weight\n",
      "features.2.0.block.0.1.weight\n",
      "features.2.0.block.0.1.bias\n",
      "features.2.0.block.1.0.weight\n",
      "features.2.0.block.1.1.weight\n",
      "features.2.0.block.1.1.bias\n",
      "features.2.0.block.2.fc1.weight\n",
      "features.2.0.block.2.fc1.bias\n",
      "features.2.0.block.2.fc2.weight\n",
      "features.2.0.block.2.fc2.bias\n",
      "features.2.0.block.3.0.weight\n",
      "features.2.0.block.3.1.weight\n",
      "features.2.0.block.3.1.bias\n",
      "features.2.1.block.0.0.weight\n",
      "features.2.1.block.0.1.weight\n",
      "features.2.1.block.0.1.bias\n",
      "features.2.1.block.1.0.weight\n",
      "features.2.1.block.1.1.weight\n",
      "features.2.1.block.1.1.bias\n",
      "features.2.1.block.2.fc1.weight\n",
      "features.2.1.block.2.fc1.bias\n",
      "features.2.1.block.2.fc2.weight\n",
      "features.2.1.block.2.fc2.bias\n",
      "features.2.1.block.3.0.weight\n",
      "features.2.1.block.3.1.weight\n",
      "features.2.1.block.3.1.bias\n",
      "features.3.0.block.0.0.weight\n",
      "features.3.0.block.0.1.weight\n",
      "features.3.0.block.0.1.bias\n",
      "features.3.0.block.1.0.weight\n",
      "features.3.0.block.1.1.weight\n",
      "features.3.0.block.1.1.bias\n",
      "features.3.0.block.2.fc1.weight\n",
      "features.3.0.block.2.fc1.bias\n",
      "features.3.0.block.2.fc2.weight\n",
      "features.3.0.block.2.fc2.bias\n",
      "features.3.0.block.3.0.weight\n",
      "features.3.0.block.3.1.weight\n",
      "features.3.0.block.3.1.bias\n",
      "features.3.1.block.0.0.weight\n",
      "features.3.1.block.0.1.weight\n",
      "features.3.1.block.0.1.bias\n",
      "features.3.1.block.1.0.weight\n",
      "features.3.1.block.1.1.weight\n",
      "features.3.1.block.1.1.bias\n",
      "features.3.1.block.2.fc1.weight\n",
      "features.3.1.block.2.fc1.bias\n",
      "features.3.1.block.2.fc2.weight\n",
      "features.3.1.block.2.fc2.bias\n",
      "features.3.1.block.3.0.weight\n",
      "features.3.1.block.3.1.weight\n",
      "features.3.1.block.3.1.bias\n",
      "features.4.0.block.0.0.weight\n",
      "features.4.0.block.0.1.weight\n",
      "features.4.0.block.0.1.bias\n",
      "features.4.0.block.1.0.weight\n",
      "features.4.0.block.1.1.weight\n",
      "features.4.0.block.1.1.bias\n",
      "features.4.0.block.2.fc1.weight\n",
      "features.4.0.block.2.fc1.bias\n",
      "features.4.0.block.2.fc2.weight\n",
      "features.4.0.block.2.fc2.bias\n",
      "features.4.0.block.3.0.weight\n",
      "features.4.0.block.3.1.weight\n",
      "features.4.0.block.3.1.bias\n",
      "features.4.1.block.0.0.weight\n",
      "features.4.1.block.0.1.weight\n",
      "features.4.1.block.0.1.bias\n",
      "features.4.1.block.1.0.weight\n",
      "features.4.1.block.1.1.weight\n",
      "features.4.1.block.1.1.bias\n",
      "features.4.1.block.2.fc1.weight\n",
      "features.4.1.block.2.fc1.bias\n",
      "features.4.1.block.2.fc2.weight\n",
      "features.4.1.block.2.fc2.bias\n",
      "features.4.1.block.3.0.weight\n",
      "features.4.1.block.3.1.weight\n",
      "features.4.1.block.3.1.bias\n",
      "features.4.2.block.0.0.weight\n",
      "features.4.2.block.0.1.weight\n",
      "features.4.2.block.0.1.bias\n",
      "features.4.2.block.1.0.weight\n",
      "features.4.2.block.1.1.weight\n",
      "features.4.2.block.1.1.bias\n",
      "features.4.2.block.2.fc1.weight\n",
      "features.4.2.block.2.fc1.bias\n",
      "features.4.2.block.2.fc2.weight\n",
      "features.4.2.block.2.fc2.bias\n",
      "features.4.2.block.3.0.weight\n",
      "features.4.2.block.3.1.weight\n",
      "features.4.2.block.3.1.bias\n",
      "features.5.0.block.0.0.weight\n",
      "features.5.0.block.0.1.weight\n",
      "features.5.0.block.0.1.bias\n",
      "features.5.0.block.1.0.weight\n",
      "features.5.0.block.1.1.weight\n",
      "features.5.0.block.1.1.bias\n",
      "features.5.0.block.2.fc1.weight\n",
      "features.5.0.block.2.fc1.bias\n",
      "features.5.0.block.2.fc2.weight\n",
      "features.5.0.block.2.fc2.bias\n",
      "features.5.0.block.3.0.weight\n",
      "features.5.0.block.3.1.weight\n",
      "features.5.0.block.3.1.bias\n",
      "features.5.1.block.0.0.weight\n",
      "features.5.1.block.0.1.weight\n",
      "features.5.1.block.0.1.bias\n",
      "features.5.1.block.1.0.weight\n",
      "features.5.1.block.1.1.weight\n",
      "features.5.1.block.1.1.bias\n",
      "features.5.1.block.2.fc1.weight\n",
      "features.5.1.block.2.fc1.bias\n",
      "features.5.1.block.2.fc2.weight\n",
      "features.5.1.block.2.fc2.bias\n",
      "features.5.1.block.3.0.weight\n",
      "features.5.1.block.3.1.weight\n",
      "features.5.1.block.3.1.bias\n",
      "features.5.2.block.0.0.weight\n",
      "features.5.2.block.0.1.weight\n",
      "features.5.2.block.0.1.bias\n",
      "features.5.2.block.1.0.weight\n",
      "features.5.2.block.1.1.weight\n",
      "features.5.2.block.1.1.bias\n",
      "features.5.2.block.2.fc1.weight\n",
      "features.5.2.block.2.fc1.bias\n",
      "features.5.2.block.2.fc2.weight\n",
      "features.5.2.block.2.fc2.bias\n",
      "features.5.2.block.3.0.weight\n",
      "features.5.2.block.3.1.weight\n",
      "features.5.2.block.3.1.bias\n",
      "features.6.0.block.0.0.weight\n",
      "features.6.0.block.0.1.weight\n",
      "features.6.0.block.0.1.bias\n",
      "features.6.0.block.1.0.weight\n",
      "features.6.0.block.1.1.weight\n",
      "features.6.0.block.1.1.bias\n",
      "features.6.0.block.2.fc1.weight\n",
      "features.6.0.block.2.fc1.bias\n",
      "features.6.0.block.2.fc2.weight\n",
      "features.6.0.block.2.fc2.bias\n",
      "features.6.0.block.3.0.weight\n",
      "features.6.0.block.3.1.weight\n",
      "features.6.0.block.3.1.bias\n",
      "features.6.1.block.0.0.weight\n",
      "features.6.1.block.0.1.weight\n",
      "features.6.1.block.0.1.bias\n",
      "features.6.1.block.1.0.weight\n",
      "features.6.1.block.1.1.weight\n",
      "features.6.1.block.1.1.bias\n",
      "features.6.1.block.2.fc1.weight\n",
      "features.6.1.block.2.fc1.bias\n",
      "features.6.1.block.2.fc2.weight\n",
      "features.6.1.block.2.fc2.bias\n",
      "features.6.1.block.3.0.weight\n",
      "features.6.1.block.3.1.weight\n",
      "features.6.1.block.3.1.bias\n",
      "features.6.2.block.0.0.weight\n",
      "features.6.2.block.0.1.weight\n",
      "features.6.2.block.0.1.bias\n",
      "features.6.2.block.1.0.weight\n",
      "features.6.2.block.1.1.weight\n",
      "features.6.2.block.1.1.bias\n",
      "features.6.2.block.2.fc1.weight\n",
      "features.6.2.block.2.fc1.bias\n",
      "features.6.2.block.2.fc2.weight\n",
      "features.6.2.block.2.fc2.bias\n",
      "features.6.2.block.3.0.weight\n",
      "features.6.2.block.3.1.weight\n",
      "features.6.2.block.3.1.bias\n",
      "features.6.3.block.0.0.weight\n",
      "features.6.3.block.0.1.weight\n",
      "features.6.3.block.0.1.bias\n",
      "features.6.3.block.1.0.weight\n",
      "features.6.3.block.1.1.weight\n",
      "features.6.3.block.1.1.bias\n",
      "features.6.3.block.2.fc1.weight\n",
      "features.6.3.block.2.fc1.bias\n",
      "features.6.3.block.2.fc2.weight\n",
      "features.6.3.block.2.fc2.bias\n",
      "features.6.3.block.3.0.weight\n",
      "features.6.3.block.3.1.weight\n",
      "features.6.3.block.3.1.bias\n",
      "features.7.0.block.0.0.weight\n",
      "features.7.0.block.0.1.weight\n",
      "features.7.0.block.0.1.bias\n",
      "features.7.0.block.1.0.weight\n",
      "features.7.0.block.1.1.weight\n",
      "features.7.0.block.1.1.bias\n",
      "features.7.0.block.2.fc1.weight\n",
      "features.7.0.block.2.fc1.bias\n",
      "features.7.0.block.2.fc2.weight\n",
      "features.7.0.block.2.fc2.bias\n",
      "features.7.0.block.3.0.weight\n",
      "features.7.0.block.3.1.weight\n",
      "features.7.0.block.3.1.bias\n",
      "features.8.0.weight\n",
      "features.8.1.weight\n",
      "features.8.1.bias\n",
      "\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 491/491 [54:14<00:00,  6.63s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1759 Acc: 0.9397\n",
      "Val   Loss: 0.1220 Acc: 0.9668\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'best_val_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[75]\u001B[39m\u001B[32m, line 54\u001B[39m\n\u001B[32m     51\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mVal   Loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mval_loss\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m Acc: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mval_acc\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m     53\u001B[39m \u001B[38;5;66;03m# ---- Early stopping ----\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m54\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m val_loss < \u001B[43mbest_val_loss\u001B[49m:\n\u001B[32m     55\u001B[39m     best_val_loss = val_loss\n\u001B[32m     56\u001B[39m     best_model_state = copy.deepcopy(model.state_dict())\n",
      "\u001B[31mNameError\u001B[39m: name 'best_val_loss' is not defined"
     ]
    }
   ],
   "execution_count": 75
  },
  {
   "cell_type": "code",
   "id": "f003312a-e387-4825-8f39-74f3d85c2d86",
   "metadata": {},
   "source": [
    "torch.save(model.state_dict(), \"efficientnet_finetuned.pth\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "91d4e131-7e36-4a70-a487-6f74daf3cb00",
   "metadata": {},
   "source": [
    "train_losses = [3.1255, 3.0977, 3.0825, 3.0588, 3.0453]\n",
    "val_losses   = [3.0977, 3.0821, 3.0709, 3.0400, 3.0357]\n",
    "train_accs   = [0.6766, 0.7041, 0.7190, 0.7424, 0.7556]\n",
    "val_accs     = [0.7031, 0.7195, 0.7302, 0.7615, 0.7654]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f9289922-afc9-4bdf-9df7-f868aca58d1b",
   "metadata": {},
   "source": [
    "epochs = [1,2,3,4,5]\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(train_losses, label=\"Train Loss\", marker='o')\n",
    "plt.plot(val_losses, label=\"Val Loss\", marker='o')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training vs Validation Loss\")\n",
    "plt.xticks(epochs)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(train_accs, label=\"Train Accuracy\", marker='o')\n",
    "plt.plot(val_accs, label=\"Val Accuracy\", marker='o')\n",
    "plt.xlim(1, 5)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Training vs Validation Accuracy\")\n",
    "plt.xticks(epochs)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5558e99c-2712-4293-b5b4-793987b579f4",
   "metadata": {},
   "source": [
    "want to shift these along so they start at epoch 1 rather than 0"
   ]
  },
  {
   "cell_type": "code",
   "id": "ca8ba4bb-887d-46f7-94dc-4145069df413",
   "metadata": {},
   "source": [
    "!python -m pip install scikit-learn\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_loader:\n",
    "        outputs = model(images)\n",
    "        preds = outputs.argmax(1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "print(classification_report(all_labels, all_preds))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "157d8163-ae05-467e-abbd-b4400ce98a9c",
   "metadata": {},
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_loader:\n",
    "        outputs = model(images)\n",
    "        preds = outputs.argmax(1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "print(classification_report(\n",
    "    all_labels, \n",
    "    all_preds,\n",
    "    zero_division = 0))\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "593ffadf-82ac-48ac-83c0-f8cf8ada486e",
   "metadata": {},
   "source": [
    "above gives classification report: precision, recall, f1 per class. now we do confusion matrix heat map"
   ]
  },
  {
   "cell_type": "code",
   "id": "3f6b0cfb-ce79-4419-8053-0c29ae4104f2",
   "metadata": {},
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "plt.figure(figsize=(12,10))\n",
    "sns.heatmap(cm, annot=False, cmap=\"Blues\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5c15f5db-5a15-4893-832a-1be56d904773",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "now time for few shot learning. shows how accuracy drops under small data conditions"
   ]
  },
  {
   "cell_type": "code",
   "id": "17002eda-a3a3-402d-8419-49f7fd8814af",
   "metadata": {},
   "source": [
    "import os\n",
    "os.getcwd()\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
